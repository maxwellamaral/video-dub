# GUIA DE INSTALA√á√ÉO E USO - PIPELINE DE DUBLAGEM LOCAL
## Com Hugging Face Transformers em GPU NVIDIA

---

## üìã PR√â-REQUISITOS

### Hardware
- **GPU NVIDIA** com pelo menos 4-6 GB de VRAM (ideal 8GB+)
- **CPU** com processador moderno
- **Armazenamento** de ~30 GB para modelos baixados

### Sistema Operacional
- Linux/WSL2 (recomendado) ou Windows nativo
- Conex√£o com a internet (para baixar modelos)

### Software Base
```bash
# Se no WSL2 ou Linux, instale ffmpeg (necess√°rio para manipular v√≠deos)
sudo apt-get update
sudo apt-get install ffmpeg

# Windows: baixe de https://ffmpeg.org/download.html
# Ou use: choco install ffmpeg (se usar Chocolatey)
```

---

## üîß INSTALA√á√ÉO DO AMBIENTE PYTHON

### 1. Criar Ambiente Virtual
```bash
# Criar ambiente
python3 -m venv venv_dublagem

# Ativar
# Linux/WSL:
source venv_dublagem/bin/activate

# Windows:
venv_dublagem\Scripts\activate
```

### 2. Instalar Depend√™ncias
```bash
# Atualizar pip, setuptools, wheel
pip install --upgrade pip setuptools wheel

# Instalar PyTorch com suporte CUDA
# Para CUDA 12.1 (compat√≠vel com drivers NVIDIA recentes):
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Ou para CUDA 11.8:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Instalar transformers, datasets e librosa
pip install transformers datasets librosa soundfile

# Opcional: para melhor performance
pip install flash-attn  # Aten√ß√£o otimizada
```

### 3. Verificar Instala√ß√£o
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA dispon√≠vel: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Nenhuma\"}')"
```

Deve retornar algo como:
```
PyTorch: 2.1.0+cu121
CUDA dispon√≠vel: True
GPU: NVIDIA GeForce RTX 3060
```

---

## üöÄ USO DO SCRIPT DE DUBLAGEM

### 1. Preparar V√≠deo
```bash
# Coloque seu v√≠deo no mesmo diret√≥rio do script e renomeie para:
# video_entrada.mp4
```

### 2. Configurar Idiomas (Opcional)
Abra `pipeline_dublagem.py` e edite essas linhas conforme necess√°rio:

```python
# Idioma original do v√≠deo
IDIOMA_ORIGEM = "en_XX"      # en_XX = ingl√™s, es_ES = espanhol, etc.

# Idioma para tradu√ß√£o e dublagem
IDIOMA_DESTINO = "pt_BR"     # pt_BR = portugu√™s brasileiro
IDIOMA_VOZ = "pt"            # Idioma da s√≠ntese de voz
```

**C√≥digos de idiomas suportados:**
- Ingl√™s: `en_XX`
- Portugu√™s Brasileiro: `pt_BR`
- Portugu√™s Europeu: `pt_PT`
- Espanhol: `es_ES`
- Franc√™s: `fr_XX`
- Alem√£o: `de_DE`
- Italiano: `it_IT`
- Japon√™s: `ja_XX`
- Chin√™s: `zh_Hans` ou `zh_Hant`
- Russo: `ru_RU`
- √Årabe: `ar_AR`
- [+ 190+ idiomas suportados pelo NLLB]

### 3. Executar Pipeline
```bash
python pipeline_dublagem.py
```

Voc√™ ver√° algo como:
```
‚úì Usando dispositivo: cuda:0
======================================================================
INICIANDO PIPELINE DE DUBLAGEM
======================================================================

üìπ Extraindo √°udio de: video_entrada.mp4
‚úì √Åudio extra√≠do: audio_extraido.wav

üéôÔ∏è  Transcrevendo √°udio com Whisper...
(Isso pode levar alguns minutos na primeira vez)
‚úì Texto transcrito (245 caracteres):
   "Hello, this is a test video for dubbing with artificial intelligence..."

üåê Traduzindo de en_XX para pt_BR...
‚úì Texto traduzido (280 caracteres):
   "Ol√°, este √© um v√≠deo de teste para dublagem com intelig√™ncia artificial..."

üîä Sintetizando fala em pt...
‚úì √Åudio sintetizado (88200 amostras)
‚úì √Åudio salvo em: audio_traduzido.wav

üé¨ Remontando v√≠deo com √°udio dublado...
‚úì V√≠deo dublado salvo em: video_dublado.mp4

======================================================================
‚úì PIPELINE COMPLETADA COM SUCESSO!
‚úì V√≠deo dublado salvo em: video_dublado.mp4
======================================================================
```

### 4. Acessar Resultado
O v√≠deo dublado estar√° em `video_dublado.mp4` no mesmo diret√≥rio.

---

## ‚öôÔ∏è AJUSTES POR PERFORMANCE

### Se ficar SEM MEM√ìRIA GPU:

**Op√ß√£o 1: Usar modelo Whisper menor**
```python
# Trocar de:
model="openai/whisper-base"

# Para:
model="openai/whisper-tiny"  # Mais r√°pido, menos preciso
# ou
model="openai/whisper-small"  # Bom balan√ßo
```

**Op√ß√£o 2: Usar float16 em vez de float32**
J√° est√° configurado por padr√£o no script.

**Op√ß√£o 3: Usar modelo NLLB menor**
```python
# Trocar de:
model="facebook/nllb-200-distilled-600M"

# Para:
model="facebook/nllb-200-distilled-600M"  # J√° √© o menor
# Se precisar de mais velocidade, usar modelo de 1.3B √© o pr√≥ximo salto
```

**Op√ß√£o 4: Processar v√≠deo em chunks**
Se o v√≠deo for muito longo (>30 min), divida em peda√ßos:
```bash
ffmpeg -i video_entrada.mp4 -c copy -segment_time 5m -f segment "chunk_%03d.mp4"
```

---

## üîä PERSONALIZA√á√ïES AVAN√áADAS

### 1. Modificar Velocidade da Fala

Encontre a fun√ß√£o `sintetizar_voz()` e adicione:
```python
# Antes de gerar, ajustar dura√ß√£o
inputs = tokenizer(text=texto, return_tensors="pt").to(DEVICE)

# Aumentar dura√ß√£o em 20%
# (requer modelo com suporte a dura√ß√£o estoc√°stica)
```

### 2. Usar Modelo TTS Alternativo (Parler-TTS)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

modelo_parler = "parler-tts/parler_tts_mini_v1"
tokenizer_parler = AutoTokenizer.from_pretrained(modelo_parler)
model_parler = AutoModelForCausalLM.from_pretrained(modelo_parler).to(DEVICE)

# Gerar fala com descri√ß√£o de speaker
description = "A 22 year old woman with a slightly high-pitched voice speaks clearly"
```

### 3. Adicionar Sincroniza√ß√£o de L√°bios (Futura)

Requer modelo adicional como `wav2lip` ou similares.

---

## üêõ RESOLU√á√ÉO DE PROBLEMAS

### Erro: "ffmpeg not found"
```bash
# Linux/WSL:
sudo apt-get install ffmpeg

# Windows (Chocolatey):
choco install ffmpeg

# Windows (Manual):
Baixe de https://ffmpeg.org/download.html e adicione ao PATH
```

### Erro: "CUDA out of memory"
‚Üí Use `model="openai/whisper-tiny"` no lugar de `base` ou `small`

### Erro: "No module named 'torch'"
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### Tradu√ß√£o com qualidade ruim
‚Üí Tente o modelo completo `facebook/nllb-200-1.3B` em vez de `-distilled-600M`
(Requer mais VRAM, ~6GB)

### Voz de s√≠ntese muito rob√≥tica
‚Üí Considere usar `parler-tts` ou `bark` para maior naturalidade
(Mais pesados em recursos)

---

## üìö PR√ìXIMOS PASSOS

### Melhorias Recomendadas:
1. **Sincroniza√ß√£o Labial**: Integrar `wav2lip` para sincronizar movimento labial
2. **M√∫ltiplas Vozes**: Detectar falantes e manter identidades de voz
3. **Processamento em Batch**: Dividir v√≠deos longos automaticamente
4. **Interface Web**: Usar Gradio ou Streamlit para tornar mais amig√°vel
5. **Cache de Modelos**: Evitar redownload de modelos j√° baixados

### Reposit√≥rios √öteis:
- [Transformers Hugging Face](https://github.com/huggingface/transformers)
- [SoniTranslate](https://github.com/R3gm/SoniTranslate)
- [Bark TTS](https://github.com/suno-ai/bark)
- [Wav2Lip](https://github.com/justinzhao/Wav2Lip_288)

---

## üìû RECURSOS ADICIONAIS

- **Documenta√ß√£o Transformers**: https://huggingface.co/docs/transformers/
- **Modelos Dispon√≠veis**: https://huggingface.co/models
- **Problemas/Issues**: https://github.com/huggingface/transformers/issues

---

## ‚öñÔ∏è AVISOS LEGAIS

- Respeite direitos autorais ao dublar conte√∫do de terceiros
- Para fins comerciais, obtenha permiss√£o do criador original
- As vozes sintetizadas podem ser detectadas como IA em algumas plataformas

