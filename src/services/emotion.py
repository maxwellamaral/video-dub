"""
Servi√ßo de An√°lise de Emo√ß√µes em √Åudio com SenseVoice.

Este m√≥dulo utiliza o modelo SenseVoiceSmall da FunAudioLLM para detectar emo√ß√µes
em segmentos de √°udio transcritos. As emo√ß√µes detectadas s√£o integradas √†s legendas
e usadas para controlar a expressividade do TTS Qwen3.

SenseVoiceSmall Features:
- Detec√ß√£o de emo√ß√µes: Angry, Happy, Sad, Neutral, etc.
- Suporte multil√≠ngue
- Alta precis√£o em Speech Emotion Recognition (SER)

Refer√™ncias:
- https://huggingface.co/FunAudioLLM/SenseVoiceSmall
- https://github.com/FunAudioLLM/SenseVoice
"""

import os
import torch
import numpy as np
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
from src.config import DEVICE

# Mapeamento de emo√ß√µes detectadas pelo SenseVoice para descri√ß√µes em portugu√™s
# utilizadas nas instru√ß√µes do Qwen3-TTS
EMOTION_MAP_PT = {
    "angry": "zangado",
    "happy": "feliz",
    "sad": "triste",
    "neutral": "neutro",
    "fearful": "amedrontado",
    "disgusted": "enojado",
    "surprised": "surpreso"
}

# Mapeamento de emo√ß√µes para instru√ß√µes detalhadas do Qwen3-TTS
# Estas instru√ß√µes orientam o modelo TTS a sintetizar com a emo√ß√£o apropriada
EMOTION_INSTRUCTIONS = {
    "angry": "Fale com tom zangado, voz elevada e ritmo acelerado, demonstrando irrita√ß√£o e frustra√ß√£o",
    "happy": "Fale com tom alegre e entusiasmado, voz animada e expressiva, transmitindo felicidade",
    "sad": "Fale com tom triste e melanc√≥lico, voz baixa e lenta, demonstrando tristeza profunda",
    "neutral": "Fale com tom neutro e equilibrado, voz clara e natural, sem √™nfase emocional",
    "fearful": "Fale com tom amedrontado e hesitante, voz tr√™mula e acelerada, demonstrando medo",
    "disgusted": "Fale com tom de desgosto e repulsa, voz √°spera, demonstrando avers√£o",
    "surprised": "Fale com tom surpreso e espantado, voz animada com inflex√µes s√∫bitas, demonstrando choque"
}


class EmotionAnalyzer:
    """
    Analisador de Emo√ß√µes em √Åudio usando SenseVoiceSmall.
    
    Detecta emo√ß√µes em segmentos de √°udio e fornece tags e instru√ß√µes
    para integra√ß√£o com legendas e s√≠ntese TTS expressiva.
    """
    
    def __init__(self, modelo="FunAudioLLM/SenseVoiceSmall", log_callback=None):
        """
        Inicializa o analisador de emo√ß√µes.
        
        Args:
            modelo (str): ID do modelo no Hugging Face ou caminho local.
            log_callback (callable, optional): Fun√ß√£o para logar mensagens.
        """
        self.modelo_nome = modelo
        self.log_callback = log_callback
        self.model = None
        self.processor = None
        
        self._carregar_modelo()
    
    def _log(self, msg):
        """Helper para logging."""
        if self.log_callback:
            self.log_callback(msg)
        else:
            print(msg)
    
    def _carregar_modelo(self):
        """
        Carrega o modelo SenseVoice e o processador.
        
        O modelo √© carregado em modo offline quando OFFLINE_MODE=True
        e utiliza GPU se dispon√≠vel para melhor desempenho.
        """
        try:
            self._log(f"   üé≠ Carregando SenseVoice: {self.modelo_nome}")
            
            from src.config import OFFLINE_MODE
            
            # Configura√ß√µes para carregamento
            load_kwargs = {
                "torch_dtype": torch.float16 if "cuda" in DEVICE else torch.float32,
                "low_cpu_mem_usage": True,
                "use_safetensors": True
            }
            
            if OFFLINE_MODE:
                load_kwargs["local_files_only"] = True
                self._log("   Modo offline ativado para SenseVoice")
            
            # Carregar processador (tokenizer + feature extractor)
            self.processor = AutoProcessor.from_pretrained(
                self.modelo_nome,
                **load_kwargs
            )
            
            # Carregar modelo
            self.model = AutoModelForSpeechSeq2Seq.from_pretrained(
                self.modelo_nome,
                **load_kwargs
            ).to(DEVICE)
            
            self.model.eval()  # Modo de infer√™ncia
            
            self._log(f"   ‚úì SenseVoice carregado em {DEVICE}")
            
        except Exception as e:
            self._log(f"   ‚úó Erro ao carregar SenseVoice: {e}")
            raise
    
    def analisar_audio(self, caminho_audio, segmentos=None):
        """
        Analisa emo√ß√µes em um arquivo de √°udio completo ou em segmentos espec√≠ficos.
        
        Args:
            caminho_audio (str): Caminho do arquivo de √°udio (.wav).
            segmentos (list, optional): Lista de segmentos com 'start', 'end', 'text'.
                                       Se None, analisa o √°udio completo.
        
        Returns:
            list ou dict: Se segmentos fornecidos, retorna lista de segmentos enriquecidos
                         com 'emotion' e 'emotion_instruction'. Caso contr√°rio, retorna
                         dict com emo√ß√£o detectada no √°udio completo.
        """
        if not os.path.exists(caminho_audio):
            self._log(f"   ‚úó √Åudio n√£o encontrado: {caminho_audio}")
            return segmentos if segmentos else {"emotion": "neutral", "confidence": 0.0}
        
        if segmentos:
            return self._analisar_segmentos(caminho_audio, segmentos)
        else:
            return self._analisar_audio_completo(caminho_audio)
    
    def _analisar_audio_completo(self, caminho_audio):
        """
        Analisa emo√ß√£o do √°udio completo.
        
        Args:
            caminho_audio (str): Caminho do arquivo de √°udio.
        
        Returns:
            dict: Dicion√°rio com 'emotion', 'confidence', 'emotion_pt', 'instruction'.
        """
        self._log(f"   üé≠ Analisando emo√ß√£o do √°udio completo...")
        
        try:
            # Carregar √°udio
            import soundfile as sf
            audio_data, sample_rate = sf.read(caminho_audio)
            
            # Converter para mono se est√©reo
            if len(audio_data.shape) > 1:
                audio_data = audio_data.mean(axis=1)
            
            # Processar √°udio
            inputs = self.processor(
                audio_data,
                sampling_rate=sample_rate,
                return_tensors="pt"
            ).to(DEVICE)
            
            # Infer√™ncia
            with torch.no_grad():
                outputs = self.model.generate(**inputs, return_dict_in_generate=True)
            
            # Decodificar resultado
            # SenseVoice retorna texto transcrito com tags de emo√ß√£o no formato <|EMOTION|>
            transcription = self.processor.batch_decode(
                outputs.sequences,
                skip_special_tokens=False
            )[0]
            
            # Extrair emo√ß√£o das tags especiais
            emotion = self._extrair_emocao_de_tag(transcription)
            
            return {
                "emotion": emotion,
                "emotion_pt": EMOTION_MAP_PT.get(emotion, "neutro"),
                "instruction": EMOTION_INSTRUCTIONS.get(emotion, ""),
                "confidence": 1.0  # SenseVoice n√£o retorna score de confian√ßa diretamente
            }
            
        except Exception as e:
            self._log(f"   ‚ö†Ô∏è Erro ao analisar emo√ß√£o: {e}")
            return {
                "emotion": "neutral",
                "emotion_pt": "neutro",
                "instruction": EMOTION_INSTRUCTIONS["neutral"],
                "confidence": 0.0
            }
    
    def _analisar_segmentos(self, caminho_audio, segmentos):
        """
        Analisa emo√ß√µes em m√∫ltiplos segmentos de √°udio.
        
        Para cada segmento, extrai o trecho de √°udio correspondente e detecta a emo√ß√£o.
        Enriquece os segmentos com informa√ß√µes emocionais para uso em legendas e TTS.
        
        Args:
            caminho_audio (str): Caminho do arquivo de √°udio completo.
            segmentos (list): Lista de dicts com 'start', 'end', 'text'.
        
        Returns:
            list: Segmentos enriquecidos com campos adicionais:
                  - 'emotion': c√≥digo da emo√ß√£o (ex: 'happy', 'sad')
                  - 'emotion_pt': emo√ß√£o em portugu√™s (ex: 'feliz', 'triste')
                  - 'emotion_instruction': instru√ß√£o para Qwen3-TTS
        """
        self._log(f"   üé≠ Analisando emo√ß√µes de {len(segmentos)} segmentos...")
        
        try:
            import soundfile as sf
            
            # Carregar √°udio completo uma vez
            audio_completo, sample_rate = sf.read(caminho_audio)
            
            # Converter para mono se necess√°rio
            if len(audio_completo.shape) > 1:
                audio_completo = audio_completo.mean(axis=1)
            
            segmentos_enriquecidos = []
            
            for i, seg in enumerate(segmentos):
                # Logar progresso a cada 10 segmentos
                if (i + 1) % 10 == 0:
                    self._log(f"   ... Analisando emo√ß√£o {i+1}/{len(segmentos)}")
                
                # Extrair trecho de √°udio do segmento
                inicio_sample = int(seg["start"] * sample_rate)
                fim_sample = int(seg["end"] * sample_rate)
                
                # Validar limites
                inicio_sample = max(0, inicio_sample)
                fim_sample = min(len(audio_completo), fim_sample)
                
                if inicio_sample >= fim_sample:
                    # Segmento inv√°lido - usar emo√ß√£o neutra
                    seg_copy = seg.copy()
                    seg_copy["emotion"] = "neutral"
                    seg_copy["emotion_pt"] = "neutro"
                    seg_copy["emotion_instruction"] = EMOTION_INSTRUCTIONS["neutral"]
                    segmentos_enriquecidos.append(seg_copy)
                    continue
                
                audio_segmento = audio_completo[inicio_sample:fim_sample]
                
                # Detectar emo√ß√£o no segmento
                emotion_data = self._detectar_emocao_trecho(audio_segmento, sample_rate)
                
                # Enriquecer segmento com dados de emo√ß√£o
                seg_enriquecido = seg.copy()
                seg_enriquecido["emotion"] = emotion_data["emotion"]
                seg_enriquecido["emotion_pt"] = emotion_data["emotion_pt"]
                seg_enriquecido["emotion_instruction"] = emotion_data["instruction"]
                
                segmentos_enriquecidos.append(seg_enriquecido)
            
            self._log(f"   ‚úì An√°lise de emo√ß√µes conclu√≠da: {len(segmentos_enriquecidos)} segmentos")
            
            return segmentos_enriquecidos
            
        except Exception as e:
            self._log(f"   ‚ö†Ô∏è Erro ao analisar segmentos: {e}")
            # Em caso de erro, retornar segmentos originais com emo√ß√£o neutra
            return [
                {**seg, "emotion": "neutral", "emotion_pt": "neutro", 
                 "emotion_instruction": EMOTION_INSTRUCTIONS["neutral"]}
                for seg in segmentos
            ]
    
    def _detectar_emocao_trecho(self, audio_data, sample_rate):
        """
        Detecta emo√ß√£o em um trecho de √°udio (numpy array).
        
        Args:
            audio_data (np.ndarray): Array numpy com dados de √°udio.
            sample_rate (int): Taxa de amostragem do √°udio.
        
        Returns:
            dict: Dicion√°rio com 'emotion', 'emotion_pt', 'instruction'.
        """
        try:
            # Processar √°udio
            inputs = self.processor(
                audio_data,
                sampling_rate=sample_rate,
                return_tensors="pt"
            ).to(DEVICE)
            
            # Infer√™ncia
            with torch.no_grad():
                outputs = self.model.generate(**inputs, return_dict_in_generate=True)
            
            # Decodificar
            transcription = self.processor.batch_decode(
                outputs.sequences,
                skip_special_tokens=False
            )[0]
            
            # Extrair emo√ß√£o
            emotion = self._extrair_emocao_de_tag(transcription)
            
            return {
                "emotion": emotion,
                "emotion_pt": EMOTION_MAP_PT.get(emotion, "neutro"),
                "instruction": EMOTION_INSTRUCTIONS.get(emotion, EMOTION_INSTRUCTIONS["neutral"])
            }
            
        except Exception as e:
            self._log(f"   ‚ö†Ô∏è Erro ao detectar emo√ß√£o em trecho: {e}")
            return {
                "emotion": "neutral",
                "emotion_pt": "neutro",
                "instruction": EMOTION_INSTRUCTIONS["neutral"]
            }
    
    def _extrair_emocao_de_tag(self, transcription):
        """
        Extrai a emo√ß√£o das tags especiais do SenseVoice.
        
        SenseVoice retorna transcri√ß√µes com tags especiais no formato:
        <|emotion|> onde emotion pode ser: HAPPY, SAD, ANGRY, NEUTRAL, etc.
        
        Args:
            transcription (str): Texto transcrito com tags especiais.
        
        Returns:
            str: C√≥digo da emo√ß√£o em min√∫sculas (ex: 'happy', 'sad', 'neutral').
        """
        import re
        
        # Padr√£o para encontrar tags de emo√ß√£o: <|EMOTION|>
        pattern = r'<\|([A-Z]+)\|>'
        matches = re.findall(pattern, transcription.upper())
        
        if matches:
            # Pegar primeira emo√ß√£o detectada
            emotion_tag = matches[0].lower()
            
            # Mapear para emo√ß√µes suportadas
            emotion_mapping = {
                "happy": "happy",
                "sad": "sad",
                "angry": "angry",
                "neutral": "neutral",
                "fear": "fearful",
                "fearful": "fearful",
                "disgust": "disgusted",
                "disgusted": "disgusted",
                "surprise": "surprised",
                "surprised": "surprised"
            }
            
            return emotion_mapping.get(emotion_tag, "neutral")
        
        # Se n√£o encontrar tag, assumir neutro
        return "neutral"
    
    def formatar_legenda_com_emocao(self, segmentos_com_emocao):
        """
        Formata segmentos enriquecidos para exibi√ß√£o em legendas.
        
        Adiciona tags de emo√ß√£o √†s legendas no formato:
        [EMO√á√ÉO] Texto da legenda
        
        Args:
            segmentos_com_emocao (list): Segmentos com campo 'emotion_pt'.
        
        Returns:
            list: Segmentos com campo 'text' formatado com tag de emo√ß√£o.
        """
        segmentos_formatados = []
        
        for seg in segmentos_com_emocao:
            seg_copy = seg.copy()
            
            # Adicionar tag de emo√ß√£o ao texto apenas se n√£o for neutro
            if seg.get("emotion", "neutral") != "neutral":
                emotion_tag = f"[{seg['emotion_pt'].upper()}]"
                seg_copy["text"] = f"{emotion_tag} {seg['text']}"
            
            segmentos_formatados.append(seg_copy)
        
        return segmentos_formatados
