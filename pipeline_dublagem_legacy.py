# ============================================================================
# PIPELINE DE DUBLAGEM DE V√çDEOS COM HUGGING FACE
# Transcri√ß√£o ‚Üí Tradu√ß√£o ‚Üí S√≠ntese de Voz ‚Üí Remontagem
# Otimizado para GPU NVIDIA
# ============================================================================

import os
import sys
import subprocess
import time
from pathlib import Path

# Detec√ß√£o precoce do ffmpeg para evitar problemas de PATH no Windows
try:
    import imageio_ffmpeg
    FFMPEG_EXE = imageio_ffmpeg.get_ffmpeg_exe()
    
    # Adicionar o diret√≥rio do ffmpeg ao PATH para que o Whisper o encontre
    ffmpeg_dir = os.path.dirname(FFMPEG_EXE)
    if ffmpeg_dir not in os.environ["PATH"]:
        os.environ["PATH"] = ffmpeg_dir + os.pathsep + os.environ["PATH"]
    
    # Garantir que exista um arquivo chamado ffmpeg.exe no diret√≥rio (algumas bibliotecas exigem esse nome exato)
    ffmpeg_base = os.path.join(ffmpeg_dir, "ffmpeg.exe")
    if not os.path.exists(ffmpeg_base):
        import shutil
        try:
            shutil.copy(FFMPEG_EXE, ffmpeg_base)
            print(f"‚úì Criado link de compatibilidade: {ffmpeg_base}")
        except Exception as e:
            print(f"‚ö†Ô∏è Aviso ao criar link de ffmpeg: {e}")

except Exception as e:
    print(f"‚ö†Ô∏è Aviso: imageio_ffmpeg falhou: {e}")
    FFMPEG_EXE = "ffmpeg"

print(f"‚úì FFMPEG Path: {FFMPEG_EXE}")

import torch
import torchaudio
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import VitsModel, AutoTokenizer as TTSTokenizer
import numpy as np
import soundfile as sf
import librosa
from moviepy import VideoFileClip, AudioFileClip, concatenate_videoclips

# Novos motores TTS
import kokoro_onnx
from TTS.api import TTS

# ============================================================================
# CONFIGURA√á√ïES INICIAIS
# ============================================================================

# Defina o dispositivo: "cuda:0" para GPU ou "cpu"
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"‚úì Usando dispositivo: {DEVICE}")

# Idiomas suportados pelo NLLB (formato: {idioma}_{script})
# Exemplos: eng_Latn (ingl√™s), por_Latn (portugu√™s), spa_Latn (espanhol)
# Lista completa: https://github.com/facebookresearch/flores/blob/main/flores200/README.md
IDIOMA_ORIGEM = "eng_Latn"      # Ingl√™s
IDIOMA_DESTINO = "por_Latn"    # Portugu√™s

# Caminhos
INPUT_DIR = "input"
os.makedirs(INPUT_DIR, exist_ok=True)
VIDEO_ENTRADA = os.path.join(INPUT_DIR, "video_entrada.mp4")      # Seu v√≠deo

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

AUDIO_EXTRAIDO = os.path.join(OUTPUT_DIR, "audio_extraido.wav")
AUDIO_REFERENCIA = os.path.join(OUTPUT_DIR, "referencia_voz.wav")
AUDIO_TRADUZIDO = os.path.join(OUTPUT_DIR, "audio_traduzido.wav")
VIDEO_SAIDA_BASE = os.path.join(OUTPUT_DIR, "video_dublado") # Sufixo ser√° adicionado
LEGENDA_ORIGINAL = os.path.join(OUTPUT_DIR, "legenda_original.srt")
LEGENDA_TRADUZIDA = os.path.join(OUTPUT_DIR, "legenda_traduzida.srt")

# Motores TTS Dispon√≠veis
MOTORES_TTS = ["mms", "coqui"]

# Modos de Encoding de V√≠deo
# 'rapido' = GPU NVENC (h264_nvenc) com preset ultrafast - muito mais r√°pido
# 'qualidade' = CPU libx264 com preset medium - melhor compress√£o
MODOS_ENCODING = ["rapido", "qualidade"]

def obter_ffmpeg_exe():
    """Retorna o caminho do execut√°vel ffmpeg."""
    try:
        return imageio_ffmpeg.get_ffmpeg_exe()
    except:
        return "ffmpeg" # Fallback para o PATH

# ============================================================================
# ETAPA 1: EXTRA√á√ÉO DE √ÅUDIO DO V√çDEO (usando ffmpeg)
# ============================================================================

def extrair_referencia_voz(caminho_video, caminho_saida, duracao=10):
    """
    Extrai os primeiros segundos do v√≠deo original para usar como refer√™ncia de clonagem.
    """
    print(f"üéôÔ∏è Extraindo refer√™ncia de voz ({duracao}s) para clonagem...")
    video = None
    try:
        from moviepy import VideoFileClip
        video = VideoFileClip(caminho_video)
        # Extrair √°udio dos primeiros 10 segundos
        trecho = video.subclipped(0, min(duracao, video.duration))
        trecho.audio.write_audiofile(caminho_saida, fps=22050, nbytes=2, codec='pcm_s16le')
        print(f"‚úì Refer√™ncia salva em: {caminho_saida}")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao extrair refer√™ncia de voz: {e}")
        return False
    finally:
        if video:
            try:
                video.close()
            except:
                pass

def extrair_audio(caminho_video, caminho_audio_saida):
    """
    Extrai √°udio do v√≠deo usando ffmpeg.
    Certifique-se de ter ffmpeg instalado: apt-get install ffmpeg (Linux/WSL)
    """
    print(f"\nüìπ Extraindo √°udio de: {caminho_video}")
    try:
        cmd = [
            FFMPEG_EXE, "-i", caminho_video,
            "-q:a", "9", "-n",  # -n n√£o sobrescreve
            caminho_audio_saida
        ]
        subprocess.run(cmd, check=True, capture_output=True)
        print(f"‚úì √Åudio extra√≠do: {caminho_audio_saida}")
        return True
    except Exception as e:
        print(f"‚úó Erro ao extrair √°udio: {e}")
        return False

# ============================================================================
# ETAPA 2: TRANSCRI√á√ÉO COM WHISPER
# ============================================================================

def transcrever_audio(caminho_audio):
    """
    Transcreve √°udio para texto usando Whisper com ALTA GRANULARIDADE.
    Retorna uma lista de segmentos otimizados para legendas.
    """
    print(f"\nüéôÔ∏è  Transcrevendo √°udio com Whisper (word-level timestamps)...")
    
    # Usar modelo slightly maior para melhor timestamp se poss√≠vel, mas base funciona
    modelo_whisper = "openai/whisper-base"
    
    pipe_speech = pipeline(
        task="automatic-speech-recognition",
        model=modelo_whisper,
        device=0 if DEVICE == "cuda:0" else -1,
        torch_dtype=torch.float16 if "cuda" in DEVICE else torch.float32,
        chunk_length_s=30,
    )
    
    # Solicitar timestamps por palavra para maior precis√£o
    # Nota: nem todos os modelos/vers√µes suportam word level perfeitamente,
    # mas o pipeline do transformers mais recente costuma suportar.
    try:
        resultado = pipe_speech(caminho_audio, return_timestamps="word")
    except Exception as e:
        print(f"‚ö†Ô∏è  Aviso: 'word' timestamps falhou ({e}), tentando 'True' padr√£o...")
        resultado = pipe_speech(caminho_audio, return_timestamps=True)

    # Extrair palavras e tempos
    raw_chunks = resultado.get("chunks", [])
    if not raw_chunks:
        # Tentar pegar do text se n√£o houver chunks
        raw_chunks = [{"text": resultado.get("text", ""), "timestamp": (0.0, 0.0)}]

    # --- Reagrupar palavras em segmentos de legenda ---
    segmentos_finais = []
    
    # Configura√ß√µes de agrupamento
    MAX_CHARS_POR_SEGMENTO = 80    # M√°ximo de caracteres por legenda
    MAX_DURACAO_SEGMENTO = 7.0     # M√°ximo de segundos por legenda
    MIN_PAUSA_QUEBRA = 0.5         # Pausa que for√ßa nova legenda
    
    buffer_palavras = []
    start_time = 0.0
    last_end_time = 0.0
    buffer_text_len = 0
    
    # Se raw_chunks vier vazio ou estranho, garantir robustez
    for i, chunk in enumerate(raw_chunks):
        # O formato do timestamp pode ser (start, end) ou dicion√°rio
        times = chunk.get("timestamp")
        text = chunk.get("text", "").strip()
        
        if not text:
            continue
            
        if isinstance(times, (list, tuple)):
            c_start, c_end = times
        else:
            c_start, c_end = last_end_time, last_end_time + 1.0

        if c_start is None: c_start = last_end_time
        if c_end is None: c_end = c_start + 0.3 # estimativa
        
        # Inicializar primeiro segmento
        if not buffer_palavras and not segmentos_finais:
            start_time = c_start
            
        # Calcular pausas
        pausa_anterior = c_start - last_end_time
        tempo_decorrido = c_end - start_time
        
        should_break = False
        
        # L√≥gica de quebra
        # 1. Se acabou de come√ßar um novo buffer
        if not buffer_palavras:
             start_time = c_start
             
        # Crit√©rios:
        if buffer_palavras:
             # Pausa longa
             if pausa_anterior > MIN_PAUSA_QUEBRA:
                 should_break = True
             # Dura√ß√£o excessiva
             elif tempo_decorrido > MAX_DURACAO_SEGMENTO:
                 should_break = True
             # Texto muito longo
             elif buffer_text_len + len(text) > MAX_CHARS_POR_SEGMENTO:
                 should_break = True
             # Pontua√ß√£o forte
             elif buffer_palavras[-1][-1] in ".?!":
                 should_break = True
        
        if should_break:
            texto_seg = " ".join(buffer_palavras)
            segmentos_finais.append({
                "start": start_time,
                "end": last_end_time,
                "text": texto_seg
            })
            buffer_palavras = []
            start_time = c_start
            buffer_text_len = 0
            
        buffer_palavras.append(text)
        buffer_text_len += len(text) + 1
        last_end_time = c_end
        
    # Adicionar o buffer restante
    if buffer_palavras:
        texto_seg = " ".join(buffer_palavras)
        segmentos_finais.append({
            "start": start_time,
            "end": last_end_time,
            "text": texto_seg
        })
        
    # Se ainda assim n√£o gerou nada, usar o texto bruto
    if not segmentos_finais:
         segmentos_finais.append({
            "start": 0.0,
            "end": last_end_time if last_end_time > 0 else 1.0,
            "text": resultado.get("text", "")
        })

    print(f"‚úì Transcri√ß√£o granular: {len(segmentos_finais)} segmentos gerados.")
    return segmentos_finais

# ============================================================================
# ETAPA 3: TRADU√á√ÉO COM NLLB (No Language Left Behind)
# ============================================================================

def traduzir_segmentos(segmentos, idioma_origem=IDIOMA_ORIGEM, idioma_destino=IDIOMA_DESTINO):
    """
    Traduz segmentos mantendo os timestamps.
    
    Args:
        segmentos: Lista de dicts com {'start': float, 'end': float, 'text': str}
        idioma_origem: C√≥digo do idioma de origem (NLLB format)
        idioma_destino: C√≥digo do idioma de destino (NLLB format)
        
    Retorna:
        list: Lista de dicts traduzidos com mesmos timestamps
    """
    print(f"\nüåê Traduzindo de {idioma_origem} para {idioma_destino}...")
    
    pipe_translation = pipeline(
        task="translation",
        model="facebook/nllb-200-distilled-600M",
        src_lang=idioma_origem,
        tgt_lang=idioma_destino,
        device=0 if DEVICE == "cuda:0" else -1,
        torch_dtype=torch.float16 if "cuda" in DEVICE else torch.float32
    )
    
    segmentos_traduzidos = []
    total = len(segmentos)
    
    print(f"   Traduzindo {total} segmentos...")
    
    for i, seg in enumerate(segmentos):
        texto = seg["text"].strip()
        if not texto:
            continue
            
        if (i + 1) % 10 == 0 or i == 0:
            print(f"   Segmento {i+1}/{total}")
        
        try:
            resultado = pipe_translation(texto, max_length=512)
            texto_traduzido = resultado[0]["translation_text"]
            
            segmentos_traduzidos.append({
                "start": seg["start"],
                "end": seg["end"],
                "text": texto_traduzido
            })
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erro no segmento {i+1}: {e}")
            # Manter o texto original em caso de erro
            segmentos_traduzidos.append({
                "start": seg["start"],
                "end": seg["end"],
                "text": texto
            })
    
    # Calcular estat√≠sticas
    texto_final = " ".join([s["text"] for s in segmentos_traduzidos])
    print(f"‚úì Texto traduzido ({len(texto_final)} caracteres, {len(segmentos_traduzidos)} segmentos)")
    print(f"   {texto_final[:100]}...")
    
    return segmentos_traduzidos


def segmentos_para_srt(segmentos):
    """
    Converte lista de segmentos para formato SRT.
    
    Args:
        segmentos: Lista de dicts com {'start': float, 'end': float, 'text': str}
        
    Retorna:
        str: Conte√∫do do arquivo SRT
    """
    def formatar_tempo_srt(segundos):
        """Converte segundos para formato SRT (HH:MM:SS,mmm)"""
        if segundos is None or segundos < 0:
            segundos = 0
        horas = int(segundos // 3600)
        minutos = int((segundos % 3600) // 60)
        secs = int(segundos % 60)
        millis = int((segundos % 1) * 1000)
        return f"{horas:02d}:{minutos:02d}:{secs:02d},{millis:03d}"
    
    linhas = []
    for i, seg in enumerate(segmentos, 1):
        inicio = formatar_tempo_srt(seg["start"])
        fim = formatar_tempo_srt(seg["end"])
        texto = seg["text"].strip()
        
        if texto:  # S√≥ adicionar se houver texto
            linhas.append(f"{i}")
            linhas.append(f"{inicio} --> {fim}")
            linhas.append(texto)
            linhas.append("")  # Linha em branco entre legendas
    
    return "\n".join(linhas)


def segmentos_para_texto(segmentos):
    """
    Extrai apenas o texto dos segmentos (para s√≠ntese de voz).
    
    Args:
        segmentos: Lista de dicts com {'start': float, 'end': float, 'text': str}
        
    Retorna:
        str: Texto concatenado
    """
    return " ".join([s["text"] for s in segmentos if s["text"].strip()])

# ============================================================================
# ETAPA 4: S√çNTESE DE VOZ (TEXT-TO-SPEECH) COM MMS-TTS
# ============================================================================

def sintetizar_voz(texto, idioma="por"):
    """
    Sintetiza texto em voz usando MMS-TTS do Facebook.
    
    Idiomas suportados (c√≥digos ISO 639-3):
    - por: portugu√™s
    - eng: ingl√™s  
    - spa: espanhol
    - fra: franc√™s
    - deu: alem√£o
    
    Modelos: facebook/mms-tts-{idioma}
    """
    print(f"\nüîä Sintetizando fala em {idioma}...")
    
    modelo_nome = f"facebook/mms-tts-{idioma}"
    
    try:
        model = VitsModel.from_pretrained(modelo_nome)
        model = model.to(DEVICE)
        tokenizer = TTSTokenizer.from_pretrained(modelo_nome)
        
        # Limpar o texto - remover caracteres especiais problem√°ticos
        import re
        texto_limpo = texto
        # Remover caracteres de controle e caracteres n√£o-imprim√≠veis
        texto_limpo = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', texto_limpo)
        # Substituir m√∫ltiplos espa√ßos por um √∫nico
        texto_limpo = re.sub(r'\s+', ' ', texto_limpo)
        # Remover caracteres especiais que podem causar problemas
        texto_limpo = re.sub(r'[^\w\s.,!?;:\-\'\"√°√†√¢√£√©√®√™√≠√¨√Æ√≥√≤√¥√µ√∫√π√ª√ß√Å√Ä√Ç√É√â√à√ä√ç√å√é√ì√í√î√ï√ö√ô√õ√á]', '', texto_limpo)
        texto_limpo = texto_limpo.strip()
        
        if not texto_limpo:
            raise ValueError("Texto vazio ap√≥s limpeza")
        
        print(f"   Texto original: {len(texto)} chars, limpo: {len(texto_limpo)} chars")
        
        # Dividir por pontua√ß√£o de fim de senten√ßa
        sentences = re.split(r'(?<=[.!?])\s+', texto_limpo)
        
        # Agrupar senten√ßas em chunks de no m√°ximo 200 caracteres
        max_chars = 200
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # Se a senten√ßa sozinha for muito grande, dividir por palavras
            if len(sentence) > max_chars:
                words = sentence.split()
                temp_chunk = ""
                for word in words:
                    if len(temp_chunk) + len(word) + 1 <= max_chars:
                        temp_chunk = (temp_chunk + " " + word).strip()
                    else:
                        if temp_chunk:
                            chunks.append(temp_chunk)
                        temp_chunk = word
                if temp_chunk:
                    if current_chunk:
                        chunks.append(current_chunk)
                        current_chunk = ""
                    chunks.append(temp_chunk)
            elif len(current_chunk) + len(sentence) + 1 <= max_chars:
                current_chunk = (current_chunk + " " + sentence).strip()
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        if not chunks:
            chunks = [texto_limpo[:max_chars]]  # Fallback
            
        print(f"   Processando {len(chunks)} chunks de texto...")
        
        all_audio = []
        for i, chunk in enumerate(chunks):
            chunk = chunk.strip()
            if not chunk or len(chunk) < 3:
                continue
            print(f"   Chunk {i+1}/{len(chunks)}: {len(chunk)} chars")
            try:
                inputs = tokenizer(text=chunk, return_tensors="pt").to(DEVICE)
                
                with torch.no_grad():
                    output = model(**inputs).waveform
                
                all_audio.append(output.cpu().numpy()[0])
            except Exception as chunk_error:
                print(f"   ‚ö†Ô∏è  Erro no chunk {i+1}, pulando: {chunk_error}")
                continue
        
        if not all_audio:
            raise ValueError("Nenhum √°udio gerado - todos os chunks falharam")
            
        # Concatenar todos os chunks
        audio_numpy = np.concatenate(all_audio)
        
        print(f"‚úì √Åudio sintetizado ({len(audio_numpy)} amostras)")
        
        return audio_numpy, model.config.sampling_rate
    
    except Exception as e:
        print(f"‚úó Erro na s√≠ntese de voz: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def salvar_audio(audio_numpy, sample_rate, caminho_saida):
    """
    Salva array numpy como arquivo WAV.
    """
    try:
        # Se for mono (1D), reshapear
        if audio_numpy.ndim == 1:
            audio_numpy = np.expand_dims(audio_numpy, axis=0)
        
        # Converter para tensor PyTorch e salvar
        waveform = torch.from_numpy(audio_numpy).float()
        
        torchaudio.save(
            caminho_saida,
            waveform,
            sample_rate=int(sample_rate)
        )
        print(f"‚úì √Åudio salvo em: {caminho_saida}")
        return True
    except Exception as e:
        print(f"‚úó Erro ao salvar √°udio: {e}")
        return False

def sintetizar_segmento_audio(texto, motor, config):
    """
    Sintetiza √°udio usando o motor escolhido (mms, kokoro, coqui).
    Retorna (audio_data_numpy, sample_rate).
    """
    if motor == "mms":
        model = config["model"]
        tokenizer = config["tokenizer"]
        device = config["device"]
        
        # Limpeza b√°sica para MMS
        texto_limpo = "".join([c for c in texto if c.isalnum() or c in " ,.?!"])
        if not texto_limpo.strip(): return None, None
        
        inputs = tokenizer(texto_limpo, return_tensors="pt").to(device)
        
        with torch.no_grad():
            output = model(**inputs).waveform
        
        audio_np = output.cpu().numpy().squeeze()
        return audio_np, model.config.sampling_rate

    elif motor == "coqui":
        tts = config["tts"]
        ref_wav = config["ref_wav"]
        lang_code = config.get("lang_coqui", "pt")
        
        # XTTS retorna lista de floats
        wav = tts.tts(text=texto, speaker_wav=ref_wav, language=lang_code)
        return np.array(wav), 24000

    return None, None

def sintetizar_batch_mms(textos, config):
    """
    Sintetiza m√∫ltiplos textos em um √∫nico lote (batch) para MMS-TTS.
    """
    model = config["model"]
    tokenizer = config["tokenizer"]
    device = config["device"]
    
    # Limpeza e filtragem
    textos_limpos = [" ".join(t.split()) for t in textos]
    textos_limpos = ["".join([c for c in t if c.isalnum() or c in " ,.?!"]) for t in textos_limpos]
    
    if not textos_limpos: return []
    
    # MMS (VITS) n√£o suporta batching nativo da mesma forma que LLMs 
    # (ele processa sequencialmente dentro do modelo se passarmos uma lista sem padding customizado)
    # No entanto, podemos otimizar o overhead de GPU chamando em sequ√™ncia ou usando batches pequenos.
    # Por agora, faremos a itera√ß√£o otimizada.
    resultados = []
    
    with torch.no_grad():
        for texto in textos_limpos:
            if not texto.strip():
                resultados.append((None, None))
                continue
            inputs = tokenizer(texto, return_tensors="pt").to(device)
            output = model(**inputs).waveform
            audio_np = output.cpu().numpy().squeeze()
            resultados.append((audio_np, model.config.sampling_rate))
            
    return resultados

def dublar_com_ajuste_video(caminho_video, segmentos, idioma_voz, saida_video, motor_tts="mms", modo_encoding="rapido"):
    """
    Vers√£o aprimorada que suporta m√∫ltiplos motores TTS e modos de encoding.
    
    Args:
        modo_encoding: 'rapido' (GPU NVENC) ou 'qualidade' (CPU libx264)
    """
    print(f"\nÔ∏è Iniciando s√≠ntese e sincroniza√ß√£o com motor: {motor_tts.upper()}")
    
    config = {"device": DEVICE, "idioma_voz": idioma_voz}
    sample_rate = None # Ser√° definido ap√≥s carregar o modelo
    
    # 1. Inicializar Motor escolhido
    try:
        if motor_tts == "mms":
            modelo_nome = f"facebook/mms-tts-{idioma_voz}"
            print(f"   Carregando MMS-TTS: {modelo_nome}")
            from transformers import VitsModel, AutoTokenizer
            config["tokenizer"] = AutoTokenizer.from_pretrained(modelo_nome)
            config["model"] = VitsModel.from_pretrained(modelo_nome).to(DEVICE)
            sample_rate = config["model"].config.sampling_rate
            
        elif motor_tts == "coqui":
            print("   Carregando Coqui XTTS v2 (pode demorar na primeira vez)...")
            # Aceitar licen√ßa automaticamente para automa√ß√£o
            os.environ["COQUI_TOS_AGREED"] = "1"
            try:
                from TTS.api import TTS
                import torch
                
                # Workaround para torch.load weights_only no PyTorch 2.6+
                # Temporariamente desabilitar weights_only para carregar modelos Coqui confi√°veis
                original_load = torch.load
                def patched_load(*args, **kwargs):
                    kwargs['weights_only'] = False
                    return original_load(*args, **kwargs)
                torch.load = patched_load
                
                print("      Inicializando modelo XTTS v2...")
                config["tts"] = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
                
                # Restaurar torch.load original
                torch.load = original_load
                
                print("      Movendo modelo para GPU...")
                config["tts"] = config["tts"].to(DEVICE)
                config["ref_wav"] = AUDIO_REFERENCIA
                config["lang_coqui"] = "pt" # XTTS usa 'pt' para portugu√™s
                sample_rate = 24000 # XTTS v2 geralmente usa 24000 Hz
                print("      ‚úì Coqui XTTS v2 carregado com sucesso!")
            except Exception as coqui_err:
                print(f"‚úó Erro ao carregar Coqui TTS: {coqui_err}")
                import traceback
                traceback.print_exc()
                return False
        else:
            print(f"‚úó Motor TTS '{motor_tts}' n√£o suportado.")
            return False
        
        if sample_rate is None:
            print("‚úó N√£o foi poss√≠vel determinar o sample_rate do motor TTS.")
            return False

    except Exception as e:
        print(f"‚úó Erro ao carregar modelo TTS: {e}")
        import traceback
        traceback.print_exc()
        return False

    video_original = None
    clips_finais = []
    arquivos_temp_audio = []
    
    try:
        # Abrir v√≠deo original uma √∫nica vez
        try:
            video_original = VideoFileClip(caminho_video)
            fps_original = video_original.fps
            print(f"   FPS do v√≠deo original: {fps_original}")
        except Exception as e:
            print(f"‚úó Erro ao abrir v√≠deo original: {e}")
            return False
            
        print(f"   Processando {len(segmentos)} segmentos...")
        
        # 2. S√≠ntese de √Åudio (Batch para MMS, Sequencial para Coqui)
        print(f"   üîä Sintetizando {len(segmentos)} segmentos de √°udio...")
        audios_sintetizados = []
        
        if motor_tts == "mms":
            textos_batch = [seg["text"] for seg in segmentos]
            audios_sintetizados = sintetizar_batch_mms(textos_batch, config)
        else:
            for seg in segmentos:
                audio_data, sr = sintetizar_segmento_audio(seg["text"], motor_tts, config)
                audios_sintetizados.append((audio_data, sr))

        # 3. Processamento de Clips de V√≠deo e Sincroniza√ß√£o
        print(f"   üé¨ Preparando clips de v√≠deo e sincronizando...")
        
        novos_segmentos_legenda = []
        tempo_acumulado = 0.0
        import soundfile as sf
        
        for i, seg in enumerate(segmentos):
            audio_data, sr = audios_sintetizados[i]
            texto = seg["text"]
            start_t = seg["start"]
            end_t = seg["end"]
            
            if start_t >= video_original.duration:
                break
                
            end_t = min(end_t, video_original.duration)
            duracao_video_orig = end_t - start_t
            if duracao_video_orig <= 0.1: continue

            video_clip = video_original.subclipped(start_t, end_t)
            duracao_final_clip = duracao_video_orig
            
            if audio_data is not None:
                temp_wav = os.path.join(OUTPUT_DIR, f"temp_batch_seg_{i}.wav")
                try:
                    # Adicionar padding de sil√™ncio (0.2s) para evitar erros de leitura al√©m do fim
                    import numpy as np
                    padding_samples = int(sr * 0.2)
                    audio_padded = np.pad(audio_data, (0, padding_samples), mode='constant')

                    sf.write(temp_wav, audio_padded, int(sr))
                    arquivos_temp_audio.append(temp_wav)
                    audio_clip = AudioFileClip(temp_wav)
                    
                    # Dura√ß√£o REAL (sem padding) para c√°lculo de speedup
                    duracao_audio_original = len(audio_data) / sr

                    ratio = duracao_video_orig / duracao_audio_original
                    ratio = max(0.1, min(ratio, 10.0)) 
                    
                    if abs(ratio - 1.0) > 0.05:
                        video_clip = video_clip.time_transform(lambda t: t * ratio)
                        duracao_final_clip = duracao_video_orig / ratio
                    else:
                        duracao_final_clip = duracao_audio_original

                    # Cortar o audio clip para a dura√ß√£o √∫til
                    audio_clip = audio_clip.with_duration(duracao_final_clip)
                    
                    video_clip = video_clip.with_audio(audio_clip)

                    # For√ßar dura√ß√£o exata do v√≠deo para bater com o √°udio √∫til
                    video_clip = video_clip.with_duration(duracao_final_clip)
                except Exception as e_audio:
                    print(f"   ‚ö†Ô∏è Erro ao processar √°udio do segmento {i}: {e_audio}")
                    video_clip = video_clip.without_audio()
            else:
                video_clip = video_clip.without_audio()
                
            video_clip = video_clip.with_fps(fps_original).with_duration(duracao_final_clip)
            clips_finais.append(video_clip)
            
            novos_segmentos_legenda.append({
                "start": tempo_acumulado,
                "end": tempo_acumulado + duracao_final_clip,
                "text": texto
            })
            tempo_acumulado += duracao_final_clip
            if (i+1) % 10 == 0: print(f"   Seg {i+1}/{len(segmentos)} processado.")

        if not clips_finais:
            print("‚úó Nenhum clip gerado para montagem.")
            return False

        print("   Concatenando clips e salvando...")
        from moviepy import concatenate_videoclips
        
        # Garantir FPS em todos os clips antes de concatenar
        clips_validados = [c.with_fps(24) if not c.fps else c for c in clips_finais]
        final_video = concatenate_videoclips(clips_validados, method="compose")
        
        tempo_inicio_enc = time.time()
        if modo_encoding == "rapido":
            # Modo R√ÅPIDO: GPU NVIDIA NVENC
            print(f"   üöÄ Modo R√ÅPIDO: Usando GPU NVENC (h264_nvenc)")
            try:
                final_video.write_videofile(
                    saida_video,
                    codec="h264_nvenc",
                    audio_codec="aac",
                    audio_bitrate="192k",
                    temp_audiofile="temp-audio.m4a",
                    remove_temp=True,
                    fps=24,
                    preset="p1",
                    ffmpeg_params=["-rc", "vbr", "-cq", "23", "-b:v", "0"],
                    logger="bar"
                )
            except Exception as e_gpu:
                print(f"   ‚ö†Ô∏è GPU NVENC falhou ou indispon√≠vel: {e_gpu}. Tentando fallback para CPU...")
                final_video.write_videofile(
                    saida_video,
                    codec="libx264",
                    audio_codec="aac",
                    audio_bitrate="192k",
                    temp_audiofile="temp-audio.m4a",
                    remove_temp=True,
                    fps=24,
                    preset="ultrafast",
                    threads=8,
                    logger="bar"
                )
        else:
            # Modo QUALIDADE: CPU libx264
            print(f"   üé¨ Modo QUALIDADE: Usando CPU libx264")
            final_video.write_videofile(
                saida_video, 
                codec="libx264", 
                audio_codec="aac", 
                audio_bitrate="192k",
                temp_audiofile="temp-audio.m4a",
                remove_temp=True,
                fps=24, 
                preset="medium", 
                threads=4, 
                ffmpeg_params=["-crf", "18"], 
                logger="bar"
            )
        
        print(f"   ‚è±Ô∏è Tempo de encoding: {time.time() - tempo_inicio_enc:.1f}s")
        
        try:
            srt_final = segmentos_para_srt(novos_segmentos_legenda)
            nome_legenda_final = os.path.join(OUTPUT_DIR, "legenda_final_sincronizada.srt")
            with open(nome_legenda_final, "w", encoding="utf-8") as f:
                f.write(srt_final)
            print(f"‚úì Legenda final sincronizada salva em: {nome_legenda_final}")
        except: pass
        
        return True
            
    except Exception as e:
        print(f"‚úó Erro na montagem final: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        print("   üßπ Limpando recursos...")
        if 'final_video' in locals() and final_video:
            try: final_video.close()
            except: pass
        for clip in clips_finais:
            try:
                if clip:
                    if hasattr(clip, 'audio') and clip.audio:
                        try: clip.audio.close()
                        except: pass
                    clip.close()
            except: pass
        if video_original:
            try: video_original.close()
            except: pass
        if arquivos_temp_audio:
            for f in arquivos_temp_audio:
                for _ in range(5):
                    try:
                        if os.path.exists(f): os.remove(f)
                        break
                    except: time.sleep(0.2)


# ============================================================================
# ETAPA 5: REMONTAGEM DE V√çDEO COM NOVO √ÅUDIO (ffmpeg)
# ============================================================================

def remontar_video(caminho_video_orig, caminho_audio_novo, caminho_saida):
    """
    Substitui a faixa de √°udio original pelo √°udio dublado.
    Usa ffmpeg para muxar v√≠deo + novo √°udio.
    """
    print(f"\nüé¨ Remontando v√≠deo com √°udio dublado...")
    
    try:
        cmd = [
            FFMPEG_EXE, "-i", caminho_video_orig,
            "-i", caminho_audio_novo,
            "-map", "0:v", "-map", "1:a",
            "-c:v", "copy", "-c:a", "aac", "-strict", "experimental",
            "-n",                     # N√£o sobrescrever
            caminho_saida
        ]
        subprocess.run(cmd, check=True, capture_output=True)
        print(f"‚úì V√≠deo dublado salvo em: {caminho_saida}")
        return True
    except Exception as e:
        print(f"‚úó Erro ao remontar v√≠deo: {e}")
        return False

# ============================================================================
# PIPELINE COMPLETA (ORQUESTRA√á√ÉO)
# ============================================================================

def executar_pipeline_completa(
    caminho_video,
    idioma_origem="eng_Latn",
    idioma_destino="por_Latn",
    idioma_voz="por",
    motor_tts="mms",
    modo_encoding="rapido"
):
    """
    Executa a pipeline completa de dublagem.
    """
    print("=" * 70)
    print(f"INICIANDO PIPELINE DE DUBLAGEM ({motor_tts.upper()})")
    print("=" * 70)
    
    # Definir nome de sa√≠da com base no motor
    saida_video = f"{VIDEO_SAIDA_BASE}_{motor_tts}.mp4"
    
    # 0. Extrair refer√™ncia de voz se for Coqui (fazemos antes para falhar cedo se necess√°rio)
    if motor_tts == "coqui":
        if not extrair_referencia_voz(caminho_video, AUDIO_REFERENCIA):
            print("‚úó Falha ao obter voz de refer√™ncia. Abortando.")
            return False

    # 1. Extrair √°udio original
    if not extrair_audio(caminho_video, AUDIO_EXTRAIDO):
        print("‚úó Falha na extra√ß√£o de √°udio. Abortando.")
        return False
    
    # 2. Transcrever (retorna segmentos)
    segmentos_originais = transcrever_audio(AUDIO_EXTRAIDO)
    if not segmentos_originais:
        print("‚úó Falha na transcri√ß√£o. Abortando.")
        return False
    
    # 2.1. Salvar legenda original (SRT) para confer√™ncia
    try:
        conteudo_srt = segmentos_para_srt(segmentos_originais)
        with open(LEGENDA_ORIGINAL, "w", encoding="utf-8") as f:
            f.write(conteudo_srt)
        print(f"‚úì Legenda original salva em: {LEGENDA_ORIGINAL}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Aviso: N√£o foi poss√≠vel salvar legenda original: {e}")
    
    # 3. Traduzir segmentos
    segmentos_traduzidos = traduzir_segmentos(segmentos_originais, idioma_origem, idioma_destino)
    if not segmentos_traduzidos:
        print("‚úó Falha na tradu√ß√£o. Abortando.")
        return False
    
    # 3.1. Salvar legenda traduzida (SRT) preliminar
    try:
        conteudo_srt_trad = segmentos_para_srt(segmentos_traduzidos)
        with open(LEGENDA_TRADUZIDA, "w", encoding="utf-8") as f:
            f.write(conteudo_srt_trad)
        print(f"‚úì Legenda traduzida salva em: {LEGENDA_TRADUZIDA}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Aviso: N√£o foi poss√≠vel salvar legenda traduzida: {e}")

    # 4. Sintetizar e Montar com ajuste din√¢mico de v√≠deo
    if not dublar_com_ajuste_video(caminho_video, segmentos_traduzidos, idioma_voz, saida_video, motor_tts, modo_encoding):
        print("‚úó Falha na s√≠ntese/montagem final. Abortando.")
        return False
    
    print("\n" + "=" * 70)
    print("‚úì PIPELINE COMPLETADA COM SUCESSO!")
    print(f"‚úì V√≠deo dublado salvo em: {saida_video}")
    print(f"‚úì Legenda original salva em: {LEGENDA_ORIGINAL}")
    print(f"‚úì Legenda final sincronizada: {os.path.join(OUTPUT_DIR, 'legenda_final_sincronizada.srt')}")
    print("=" * 70)
    
    return True

def obter_configuracao_usuario():
    """Exibe um menu para o usu√°rio escolher os idiomas e o motor de dublagem."""
    opcoes_idioma = [
        {"nome": "Ingl√™s para Portugu√™s (Brasil)", "origem": "eng_Latn", "destino": "por_Latn", "voz": "por"},
        {"nome": "Portugu√™s para Ingl√™s", "origem": "por_Latn", "destino": "eng_Latn", "voz": "eng"},
        {"nome": "Espanhol para Portugu√™s (Brasil)", "origem": "spa_Latn", "destino": "por_Latn", "voz": "por"},
        {"nome": "Personalizado", "origem": "manual", "destino": "manual", "voz": "manual"},
        {"nome": "Sair", "origem": "exit", "destino": "exit", "voz": "exit"}
    ]

    print("\n" + "="*50)
    print("       1. CONFIGURA√á√ÉO DE IDIOMAS")
    print("="*50)
    for i, opcao in enumerate(opcoes_idioma, 1):
        print(f"{i}. {opcao['nome']}")
    
    try:
        escolha_idioma = int(input("\nEscolha os idiomas (padr√£o 1): ") or "1")
        config_id = opcoes_idioma[escolha_idioma-1]
        
        if config_id["origem"] == "exit": return None, None, None, None
        
        origem, destino, voz = config_id["origem"], config_id["destino"], config_id["voz"]
        if origem == "manual":
            origem = input("C√≥digo NLLB Origem (ex: eng_Latn): ") or "eng_Latn"
            destino = input("C√≥digo NLLB Destino (ex: por_Latn): ") or "por_Latn"
            voz = input("C√≥digo MMS-TTS Voz (ex: por): ") or "por"

    except (ValueError, IndexError):
        origem, destino, voz = "eng_Latn", "por_Latn", "por"

    print("\n" + "="*50)
    print("       2. MECANISMO DE VOZ (TTS)")
    print("="*50)
    print("1. MMS-TTS (Padr√£o, Offline, Leve)")
    print("2. Coqui XTTS v2 (Clonagem de Voz do V√≠deo)")
    
    try:
        escolha_tts = int(input("\nEscolha o motor (padr√£o 1): ") or "1")
        motores = {1: "mms", 2: "coqui"}
        motor = motores.get(escolha_tts, "mms")
    except ValueError:
        motor = "mms"

    print("\n" + "="*50)
    print("       3. MODO DE ENCODING (V√çDEO)")
    print("="*50)
    print("1. R√°pido (GPU NVENC - Necess√°rio NVIDIA)")
    print("2. Qualidade (CPU libx264 - Melhor compress√£o)")
    
    try:
        escolha_enc = int(input("\nEscolha o modo (padr√£o 1): ") or "1")
        modos = {1: "rapido", 2: "qualidade"}
        modo = modos.get(escolha_enc, "rapido")
    except ValueError:
        modo = "rapido"

    return origem, destino, voz, motor, modo

if __name__ == "__main__":
    # Certifique-se de que seu v√≠deo existe
    if not os.path.exists(VIDEO_ENTRADA):
        print(f"\n[!] Arquivo n√£o encontrado: {VIDEO_ENTRADA}")
        print(f"    Por favor, coloque seu v√≠deo na pasta '{INPUT_DIR}' e renomeie para 'video_entrada.mp4'")
    else:
        # Obter configura√ß√µes
        res = obter_configuracao_usuario()
        if res[0] is None:
            print("\nüëã Saindo...")
            exit()
            
        origem, destino, voz, motor, modo = res
        
        # Executar pipeline completa
        sucesso = executar_pipeline_completa(
            caminho_video=VIDEO_ENTRADA,
            idioma_origem=origem,
            idioma_destino=destino,
            idioma_voz=voz,
            motor_tts=motor,
            modo_encoding=modo
        )
        
        if sucesso:
            print("\nüí° Dicas:")
            print("  ‚Ä¢ Para v√≠deos mais longos, considere usar modelo 'tiny' no Whisper")
            print("  ‚Ä¢ Se ficar sem mem√≥ria, reduza para float16 ou use modelos menores")
            print("  ‚Ä¢ Edite os c√≥digos de idioma conforme necess√°rio no topo do arquivo")

# ============================================================================
# REFER√äNCIAS DE IDIOMAS (NLLB e MMS-TTS)
# ============================================================================

# C√≥digos NLLB (Flores-200 - amostra):
# eng_Latn = ingl√™s
# por_Latn = portugu√™s
# spa_Latn = espanhol
# fra_Latn = franc√™s
# deu_Latn = alem√£o
# ita_Latn = italiano
# jpn_Jpan = japon√™s
# zho_Hans = chin√™s simplificado
# rus_Cyrl = russo
# ara_Arab = √°rabe

# Idiomas MMS-TTS (ISO 639-3):
# por = portugu√™s
# eng = ingl√™s
# spa = espanhol
# fra = franc√™s
# deu = alem√£o
# ita = italiano
# jpn = japon√™s
# cmn = chin√™s (mandarim)
# rus = russo
# ara = √°rabe
